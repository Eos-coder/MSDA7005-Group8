{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9561dda8-b2d8-4bdf-98be-f0ba66364640",
   "metadata": {},
   "source": [
    "1. Final script crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from DrissionPage import ChromiumPage, WebPage\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c7bd061b1c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the crawling of Xiaohongshu requires logging in with an account, we have customized a sign in function.\n",
    "def sign_in():\n",
    "    sign_in_page = ChromiumPage()\n",
    "    sign_in_page.get('https://www.xiaohongshu.com')\n",
    "    print(\"please scan the qr-code to sign in\")\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4217958de6a3e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the keyword into a format that can be read as a link\n",
    "def search(keyword):\n",
    "    global page\n",
    "    page = ChromiumPage()\n",
    "    page.get(f'https://www.xiaohongshu.com/search_result?keyword={quote(keyword)}&source=web_search_result_notes')\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce2bd75ab154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that grabs a link to a post that appears in the search results screen.\n",
    "def get_links():\n",
    "    global links\n",
    "    links = []\n",
    "    try:\n",
    "        container = page.ele('.feeds-page')\n",
    "        sections = container.eles('.note-item')\n",
    "        for section in sections:\n",
    "            soup = BeautifulSoup(section.html, 'html.parser')\n",
    "            note_link_element = soup.find('a', class_='cover ld mask')\n",
    "            href = note_link_element.get('href')  \n",
    "            if href:  \n",
    "                note_link = \"https://www.xiaohongshu.com\" + href\n",
    "                links.append(note_link)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_links: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b68f79520a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xiaohongshu's search interface is not loaded all at once, so we need to add a function that simulates scrolling down.\n",
    "def page_scroll_down():\n",
    "    print(\"********scroll down the page********\")\n",
    "    random_time = random.uniform(0.5, 1.5)\n",
    "    time.sleep(random_time)\n",
    "    page.scroll.to_bottom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739aaa6b57c1a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After sliding down, Xiaohongshu still needs some time to load the content, otherwise it may not be able to crawl the content.\n",
    "def craw(times):\n",
    "    for i in tqdm(range(1, times + 1)):\n",
    "        get_links()\n",
    "        page_scroll_down()\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929173ee7198751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl post links for ten keywords\n",
    "keywords = [\"婚姻\", \"结婚\", \"婚姻观\", \"结婚的意义\", \"婚姻关系\", \"不婚\", \"未婚\", \"已婚\", \"夫妻\", \"婚姻家庭\"]\n",
    "# A search term interface usually needs to be scrolled down 20 times to reach the bottom of the page.\n",
    "times = 20\n",
    "\n",
    "sign_in()\n",
    "df1 = []\n",
    "\n",
    "for keyword in keywords:\n",
    "    search(keyword)\n",
    "    craw(times)\n",
    "    # combine crawled post links and corresponding search keywords into a data frame\n",
    "    data_for_keyword = pd.DataFrame(links, columns=['note_link'])\n",
    "    data_for_keyword['source_keyword'] = keyword  \n",
    "    df1.append(data_for_keyword)\n",
    "link_df = pd.concat(df1, ignore_index=True) \n",
    "#print(link_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa44e5d050789d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_url(url):\n",
    "    global page\n",
    "    # Xiaohongshu will enter the slider verification(=滑块验证) module if too much is scraped at once\n",
    "    page = WebPage('s')\n",
    "    page.get(f'{url}')\n",
    "    if page.title == '滑块验证':\n",
    "        page.change_mode()\n",
    "        page.get(f'{url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfab12fa74f7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_info(page):\n",
    "    # Locate author information\n",
    "    div_author = page.ele('.author-container', timeout=0)\n",
    "    div_info = div_author.ele('.info', timeout=0)\n",
    "    author_name = div_info.ele('.username', timeout=0).text\n",
    "    author_info = {'author_name': author_name}\n",
    "    return author_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24189dbb879f3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_note_content(page):\n",
    "    # Position the div containing the note details\n",
    "    note_content = page.ele('.note-content', timeout=0)\n",
    "\n",
    "    # title\n",
    "    try:\n",
    "        note_title = note_content.ele('.title', timeout=0).text\n",
    "    except:\n",
    "        note_title = \"\"\n",
    "\n",
    "    # description= post content\n",
    "    try:\n",
    "        note_desc = note_content.ele('.desc', timeout=0).text\n",
    "    except:\n",
    "        note_desc = \"\"\n",
    "\n",
    "    # tag\n",
    "    tags = []\n",
    "    try:\n",
    "        note_tags = note_content.eles('.tag', timeout=0)\n",
    "        for tag in note_tags:\n",
    "            tag_text = tag.texts()[0]\n",
    "            tags.append(tag_text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # publish data\n",
    "    note_date = note_content.ele('.bottom-container', timeout=0).text\n",
    "    # Extract date\n",
    "    if \"编辑于\" in note_date:\n",
    "        parts = note_date.split(\" \")\n",
    "        date = parts[1]\n",
    "        count = date.count('-')\n",
    "        if count == 1:\n",
    "            # fill the year\n",
    "            current_year = datetime.now().year\n",
    "            date = str(current_year) + \"-\" + date\n",
    "    else:\n",
    "        date = note_date\n",
    "        count = date.count('-')\n",
    "        if count == 1:\n",
    "            current_year = datetime.now().year\n",
    "            date = str(current_year) + \"-\" + date\n",
    "\n",
    "    content = {'note_title': note_title, 'note_desc': note_desc,\n",
    "               'tags': tags, 'note_date': date}\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c9b83c5a34c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(page):\n",
    "    \"\"\"Get Likes, Collections, Comments counts\"\"\"\n",
    "    html = page.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    like_count = soup.find_all('meta', attrs={'name': 'og:xhs:note_like'})[0]['content']\n",
    "    collect_count = soup.find_all('meta', attrs={'name': 'og:xhs:note_collect'})[0]['content']\n",
    "    chat_count = soup.find_all('meta', attrs={'name': 'og:xhs:note_comment'})[0]['content']\n",
    "\n",
    "    count = {'like_count': like_count, 'collect_count': collect_count, 'chat_count': chat_count}\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3bc10f754b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_note_page_info(url):\n",
    "    open_url(url)\n",
    "    try:\n",
    "        # extract the author info& note details &counts\n",
    "        author_info = get_author_info(page)\n",
    "        content = get_note_content(page)\n",
    "        count = get_count(page)\n",
    "\n",
    "        # extract information  \n",
    "        author_name = author_info['author_name']\n",
    "        note_title = content['note_title']\n",
    "        note_desc = content['note_desc']\n",
    "        tags = content['tags']\n",
    "        date_str = content['note_date']\n",
    "        like_count = count['like_count']\n",
    "        collect_count = count['collect_count']\n",
    "        comment_count = count['chat_count']\n",
    "\n",
    "        # construct the data dictionary to be returned, unify column names, and fill in note_url and source_keyword\n",
    "        note_info_dict = {'title': note_title, 'content': note_desc, 'time': date_str,\n",
    "                          'author_name': author_name, 'liked_count': like_count,\n",
    "                          'collected_count': collect_count, 'comment_count': comment_count,\n",
    "                          'tag_list': tags, 'note_url': url, 'source_keyword': \"\"}\n",
    "        return note_info_dict\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4afade5cca6f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl the required information from the URLs in the link_df, such as titles, content, etc.\n",
    "all_note_info = []\n",
    "for index, row in link_df.iterrows():\n",
    "    note_info = get_note_page_info(row[\"note_link\"])\n",
    "    if note_info:\n",
    "        # extract the note_url and source_keyword column data to fill in the corresponding positions\n",
    "        note_info['source_keyword'] = row['source_keyword']\n",
    "        all_note_info.append(note_info)\n",
    "\n",
    "result_df = pd.DataFrame(all_note_info)\n",
    "#print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831129fff302d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a CSV file with a lowercase filename\n",
    "result_df.to_csv('xhs_content.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
